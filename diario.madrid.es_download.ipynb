{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: latin-1 -*-\n",
    "\n",
    "import inspect, os, sys\n",
    "\n",
    "try :\n",
    "    import pywikibot as pb\n",
    "    from pywikibot.specialbots import UploadRobot\n",
    "\n",
    "except :\n",
    "    current_folder = os.path.realpath(os.path.abspath(os.path.split(inspect.getfile(inspect.currentframe()))[0]))\n",
    "    folder_parts = current_folder.split(os.sep)\n",
    "    pywikibot_folder = os.sep.join(folder_parts[:-1])\n",
    "\n",
    "    if current_folder not in sys.path:\n",
    "        sys.path.insert(0, current_folder)\n",
    "    if pywikibot_folder not in sys.path:\n",
    "        sys.path.insert(0, pywikibot_folder)\n",
    "\n",
    "    import pywikibot as pb\n",
    "    from pywikibot.specialbots import UploadRobot\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from mako.template import Template\n",
    "import requests\n",
    "from requests.compat import quote\n",
    "from PIL import Image, ImageFile\n",
    "from openpyxl import Workbook\n",
    "\n",
    "import calendar\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "from modules.wmtools import is_commons_file, get_hash, sanitize_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dimensions (uri, headers) :\n",
    "    image_url = quote(uri.encode('utf-8'), ':/')\n",
    "    try: \n",
    "        r = requests.get(image_url, headers=headers)\n",
    "        image = Image.open(BytesIO(r.content))\n",
    "        return image.size[0] * image.size[1]\n",
    "    except :\n",
    "        print ('Exception')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commons_site = pb.Site(\"commons\", \"commons\")\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "EXCEL_FILE = \"diario.madrid.es.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "images_directory = os.path.join(cwd, 'images')\n",
    "if not os.path.exists(images_directory):\n",
    "    os.makedirs(images_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "headers = {'User-Agent': user_agent}\n",
    "url_base = 'https://diario.madrid.es/blog/notas-de-prensa/'\n",
    "\n",
    "r = requests.get(url_base, headers=headers)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "pages = soup.find_all(\"a\", class_=\"page-numbers\")\n",
    "final_page = int(pages[-2].get_text())-1\n",
    "final_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number = 100\n",
    "row = []\n",
    "for i in range (final_page) :\n",
    "    if i > page_number :\n",
    "        break\n",
    "\n",
    "    if i == 0 :\n",
    "        url = url_base\n",
    "    else :\n",
    "        url = '{0}page/{1}'.format(url_base, i+1)\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all(\"h2\", class_=\"post-title\")\n",
    "    for article in articles :\n",
    "        article_url = article.a[\"href\"]\n",
    "        print('-----')\n",
    "        print(article_url)\n",
    "        r = requests.get(article_url, headers=headers)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        title = soup.find_all(\"h1\", class_=\"post-title\")[0].get_text().strip().replace('  ', ' ')\n",
    "        print (title)\n",
    "        date = '-'.join(soup.find_all(\"div\", class_=\"post-date\")[0].get_text().strip().split('/')[::-1])\n",
    "        year = date.split('-')[0]\n",
    "        month = calendar.month_name[int(date.split('-')[1])]\n",
    "        body = soup.find_all(\"div\", class_=\"post-content\")\n",
    "        p_description = body[0].find_all(\"p\")\n",
    "        #print (p_description)\n",
    "        for p in p_description :\n",
    "            if len(p.get_text()) > 10:\n",
    "                description = p.get_text()\n",
    "                break\n",
    "\n",
    "        counter = 0\n",
    "        images = [image.a[\"href\"] for image in soup.find_all(\"div\", class_=\"gallery-icon\")]\n",
    "        for image in images:\n",
    "            try:\n",
    "                image_url = quote(image.encode('utf-8'), ':/')\n",
    "            except :\n",
    "                continue\n",
    "            print (image_url)\n",
    "            image_name = sanitize_filename('{} {:02d}.jpg'.format(title, counter+1))\n",
    "            image_path = os.path.join(images_directory, image_name)\n",
    "            print (image_path)\n",
    "            try:\n",
    "                r = requests.get(image_url, headers=headers)\n",
    "                image = Image.open(BytesIO(r.content))\n",
    "                image.save(image_path)\n",
    "                row = [image_path, image_url, title, article_url, date, year, month, description]\n",
    "                ws.append(row)\n",
    "                pass\n",
    "            except :\n",
    "                print ('Failed download. Skipping')\n",
    "            counter += 1\n",
    "\n",
    "        images = soup.find_all(attrs={'class': re.compile(r\"wp-image-\\d*\")})\n",
    "        for i, image in enumerate(images) :\n",
    "            try:\n",
    "                links = {int(x.strip().split(' ')[1][:-1]): x.strip().split(' ')[0] for x in image.get('srcset').split(',')}\n",
    "                sortedlist = [(k, links[k]) for k in sorted(links)]\n",
    "                link = sortedlist[-1][1]\n",
    "            except :\n",
    "                link = image.get('src')\n",
    "                \n",
    "            if image.parent.get('href') == None :\n",
    "                # No parent\n",
    "                print ('No parent')\n",
    "                image_url = link\n",
    "                image_url = quote(image_url.encode('utf-8'), ':/')\n",
    "                print (image_url)\n",
    "            elif get_image_dimensions(image.parent.get('href'), headers) > get_image_dimensions(link, headers) :\n",
    "                # Bigger size\n",
    "                print ('Bigger size')\n",
    "                image_url = image.parent.get('href')\n",
    "                image_url = quote(image_url.encode('utf-8'), ':/')\n",
    "                print (image_url)\n",
    "            else :\n",
    "                # Others\n",
    "                print ('Other')\n",
    "                image_url = link\n",
    "                image_url = quote(image_url.encode('utf-8'), ':/')\n",
    "                print (image_url)\n",
    "            image_name = sanitize_filename('{} {:02d}.jpg'.format(title, counter+1))\n",
    "            image_path = os.path.join(images_directory, image_name)\n",
    "            print (image_path)\n",
    "\n",
    "            #print (image_path)\n",
    "            try:\n",
    "                r = requests.get(image_url, headers=headers)\n",
    "                image = Image.open(BytesIO(r.content))\n",
    "                image.save(image_path)\n",
    "                row = [image_path, image_url, title, article_url, date, year, month, description]\n",
    "                ws.append(row)\n",
    "                pass\n",
    "            except :\n",
    "                print ('Failed download. Skipping')\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb.save(os.path.join(cwd, EXCEL_FILE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
